# Knowledge Base Multi-Source Support Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Add PowerPoint (.pptx), Website URLs, and YouTube URLs support to Knowledge Base with unified interface that auto-detects source type.

**Architecture:** Extend existing Knowledge Base to accept mixed sources (files + URLs). Backend uses Agno's built-in readers (pptx_reader, website_reader, youtube_reader) and auto-detects source type by pattern matching (file path vs http/https vs youtube.com). Frontend remains file-upload focused initially (URLs added in future iteration per YAGNI).

**Tech Stack:** FastAPI (upload API), Agno Knowledge class (multi-format readers), React (future URL input), pgvector (existing)

---

## Task 1: Backend - Add PowerPoint (.pptx) Support

**Files:**
- Modify: `backend/app/api/upload.py:22`
- Test: `backend/tests/test_api.py` (add new test)

**Step 1: Write the failing test**

Add to `backend/tests/test_api.py`:

```python
class TestPowerPointUpload:
    """Test PowerPoint file upload support."""

    def test_upload_pptx_file(self, client):
        """PowerPoint files should be accepted."""
        # Create minimal valid PPTX (ZIP archive with required structure)
        import zipfile
        import io

        pptx_buffer = io.BytesIO()
        with zipfile.ZipFile(pptx_buffer, 'w') as zf:
            zf.writestr('[Content_Types].xml', '<?xml version="1.0"?><Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types"></Types>')
            zf.writestr('_rels/.rels', '<?xml version="1.0"?><Relationships xmlns="http://schemas.openxmlformats.org/package/2006/relationships"></Relationships>')

        pptx_content = pptx_buffer.getvalue()

        response = client.post(
            "/api/upload/",
            files={"file": ("test.pptx", pptx_content, "application/vnd.openxmlformats-officedocument.presentationml.presentation")},
        )

        assert response.status_code == 200
        data = response.json()
        assert data["extension"] == ".pptx"
        assert data["file_type"] == "text"  # Validated as readable
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_api.py::TestPowerPointUpload::test_upload_pptx_file -v`
Expected: FAIL with "File type '.pptx' not allowed"

**Step 3: Add .pptx to allowed extensions**

In `backend/app/api/upload.py`, update line 22:

```python
ALLOWED_EXTENSIONS = {".pdf", ".txt", ".md", ".csv", ".json", ".docx", ".pptx"}
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_api.py::TestPowerPointUpload::test_upload_pptx_file -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/api/upload.py backend/tests/test_api.py
git commit -m "feat: add PowerPoint (.pptx) file upload support

- Added .pptx to ALLOWED_EXTENSIONS
- Test validates PPTX structure and upload
- Agno pptx_reader auto-detects and processes slides"
```

---

## Task 2: Backend - Add Source Type Detection Utility

**Files:**
- Create: `backend/app/services/source_detector.py`
- Test: `backend/tests/test_source_detector.py`

**Step 1: Write the failing test**

Create `backend/tests/test_source_detector.py`:

```python
"""Tests for source type detection."""
import pytest
from app.services.source_detector import SourceDetector, SourceType


def test_detect_file_path():
    """Local file paths should be detected as FILE."""
    assert SourceDetector.detect("/path/to/document.pdf") == SourceType.FILE
    assert SourceDetector.detect("./relative/path/doc.txt") == SourceType.FILE
    assert SourceDetector.detect("uploads/file.pptx") == SourceType.FILE


def test_detect_website_url():
    """HTTP/HTTPS URLs should be detected as WEBSITE."""
    assert SourceDetector.detect("https://example.com/page") == SourceType.WEBSITE
    assert SourceDetector.detect("http://docs.python.org") == SourceType.WEBSITE


def test_detect_youtube_url():
    """YouTube URLs should be detected as YOUTUBE."""
    assert SourceDetector.detect("https://youtube.com/watch?v=dQw4w9WgXcQ") == SourceType.YOUTUBE
    assert SourceDetector.detect("https://www.youtube.com/watch?v=abc123") == SourceType.YOUTUBE
    assert SourceDetector.detect("https://youtu.be/dQw4w9WgXcQ") == SourceType.YOUTUBE


def test_detect_invalid_source():
    """Invalid sources should raise ValueError."""
    with pytest.raises(ValueError, match="Invalid source"):
        SourceDetector.detect("")

    with pytest.raises(ValueError, match="Invalid source"):
        SourceDetector.detect("   ")
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_source_detector.py -v`
Expected: FAIL with "ModuleNotFoundError: No module named 'app.services.source_detector'"

**Step 3: Write minimal implementation**

Create `backend/app/services/source_detector.py`:

```python
"""Source type detection for Knowledge Base inputs."""
from enum import Enum
from typing import Optional


class SourceType(Enum):
    """Types of sources supported by Knowledge Base."""
    FILE = "file"
    WEBSITE = "website"
    YOUTUBE = "youtube"


class SourceDetector:
    """Detects source type from path/URL string."""

    @staticmethod
    def detect(source: str) -> SourceType:
        """Detect source type from string.

        Args:
            source: File path, website URL, or YouTube URL

        Returns:
            SourceType enum value

        Raises:
            ValueError: If source is empty or invalid
        """
        if not source or not source.strip():
            raise ValueError("Invalid source: empty string")

        source = source.strip()

        # Check for YouTube URLs
        if "youtube.com" in source or "youtu.be" in source:
            return SourceType.YOUTUBE

        # Check for HTTP/HTTPS URLs (website)
        if source.startswith("http://") or source.startswith("https://"):
            return SourceType.WEBSITE

        # Default to file path
        return SourceType.FILE
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_source_detector.py -v`
Expected: PASS (all 4 tests)

**Step 5: Commit**

```bash
git add backend/app/services/source_detector.py backend/tests/test_source_detector.py
git commit -m "feat: add source type detection utility

- Detects FILE, WEBSITE, or YOUTUBE from source string
- Pattern matching: YouTube domains, HTTP(S) URLs, file paths
- Comprehensive test coverage for all source types"
```

---

## Task 3: Backend - Update Execution Engine for Multi-Source Loading

**Files:**
- Modify: `backend/app/services/execution_engine.py:241-247`
- Test: `backend/tests/test_kb_multi_source.py` (new)

**Step 1: Write the failing test**

Create `backend/tests/test_kb_multi_source.py`:

```python
"""Tests for Knowledge Base with multiple source types."""
import os
import pytest
from app.services.source_detector import SourceType, SourceDetector


def test_source_type_detection():
    """Verify source type detection logic."""
    # File paths
    assert SourceDetector.detect("/uploads/doc.pdf") == SourceType.FILE

    # Website URLs
    assert SourceDetector.detect("https://python.org") == SourceType.WEBSITE

    # YouTube URLs
    assert SourceDetector.detect("https://youtube.com/watch?v=abc") == SourceType.YOUTUBE


@pytest.mark.skipif(
    not os.environ.get("DATABASE_URL") or not os.environ.get("OPENAI_API_KEY"),
    reason="Requires DATABASE_URL and OPENAI_API_KEY for integration testing"
)
@pytest.mark.asyncio
async def test_knowledge_base_loads_file_source():
    """Test that Knowledge Base loads file sources via add_content_async."""
    from agno.knowledge import Knowledge
    from agno.vectordb.pgvector import PgVector

    # Create Knowledge instance
    vector_db = PgVector(
        table_name="kb_test",
        db_url=os.environ["DATABASE_URL"]
    )
    knowledge = Knowledge(name="kb_test", vector_db=vector_db)

    # Create a test file
    test_file = "/tmp/test_kb_file.txt"
    with open(test_file, "w") as f:
        f.write("Test content for knowledge base.")

    # Load file asynchronously
    await knowledge.add_content_async(path=test_file)

    # Verify content was added (knowledge should have documents)
    # Note: Agno's Knowledge class doesn't expose document count directly
    # This test verifies no exception was raised
    assert True

    # Cleanup
    os.remove(test_file)


@pytest.mark.skip(reason="YouTube reader requires youtube-transcript-api package")
@pytest.mark.asyncio
async def test_knowledge_base_loads_youtube_source():
    """Test that Knowledge Base can load YouTube URLs (placeholder)."""
    # This will be implemented when youtube-transcript-api is added
    pass


@pytest.mark.skip(reason="Website reader requires requests/beautifulsoup packages")
@pytest.mark.asyncio
async def test_knowledge_base_loads_website_source():
    """Test that Knowledge Base can load website URLs (placeholder)."""
    # This will be implemented when web scraping deps are added
    pass
```

**Step 2: Run test to verify current state**

Run: `cd backend && OPENAI_API_KEY=$(grep OPENAI_API_KEY .env | cut -d= -f2) DATABASE_URL=$(grep DATABASE_URL .env | cut -d= -f2) pytest tests/test_kb_multi_source.py::test_knowledge_base_loads_file_source -v`
Expected: PASS (current code already supports file paths)

**Step 3: Update execution engine to detect source types**

In `backend/app/services/execution_engine.py`, replace lines 241-247:

```python
        # Load documents asynchronously if sources are provided
        sources = compiled.get("sources", [])
        if sources:
            from app.services.source_detector import SourceDetector, SourceType

            logger.info(f"Loading {len(sources)} sources into knowledge base...")
            for source in sources:
                source_type = SourceDetector.detect(source)

                if source_type == SourceType.FILE:
                    await knowledge.add_content_async(path=source)
                elif source_type == SourceType.WEBSITE:
                    await knowledge.add_content_async(url=source)
                elif source_type == SourceType.YOUTUBE:
                    await knowledge.add_content_async(url=source)

                logger.info(f"Loaded {source_type.value}: {source}")

            logger.info("All sources loaded successfully")
```

**Step 4: Run test to verify it still passes**

Run: `cd backend && OPENAI_API_KEY=$(grep OPENAI_API_KEY .env | cut -d= -f2) DATABASE_URL=$(grep DATABASE_URL .env | cut -d= -f2) pytest tests/test_kb_multi_source.py::test_knowledge_base_loads_file_source -v`
Expected: PASS

**Step 5: Run all existing tests to ensure no regression**

Run: `cd backend && pytest tests/ -v -k "not youtube and not website"`
Expected: All tests PASS (skip YouTube/Website tests until deps added)

**Step 6: Commit**

```bash
git add backend/app/services/execution_engine.py backend/tests/test_kb_multi_source.py
git commit -m "feat: add multi-source type detection to execution engine

- Execution engine now detects FILE vs WEBSITE vs YOUTUBE
- Uses add_content_async(path=...) for files
- Uses add_content_async(url=...) for websites/YouTube
- Placeholder tests for website/YouTube (pending deps)"
```

---

## Task 4: Backend - Add Dependencies for Website and YouTube Readers

**Files:**
- Modify: `backend/requirements.txt`
- Test: Manual verification

**Step 1: Check current Agno readers availability**

Run: `cd backend && python3 -c "from agno.knowledge import Knowledge; k = Knowledge(); print('Website reader:', hasattr(k, 'website_reader')); print('YouTube reader:', hasattr(k, 'youtube_reader'))"`
Expected: Both print `True` (readers exist in Agno)

**Step 2: Test if readers work without additional deps**

Run: `cd backend && python3 -c "from agno.knowledge import Knowledge; k = Knowledge(); print(k.website_reader); print(k.youtube_reader)"`
Expected: May show warnings or errors about missing dependencies (requests, beautifulsoup4, youtube-transcript-api)

**Step 3: Add optional dependencies to requirements.txt**

Append to `backend/requirements.txt`:

```txt
# Knowledge Base multi-source support
beautifulsoup4>=4.12.0  # Website content extraction
requests>=2.31.0         # HTTP requests for website reader
youtube-transcript-api>=0.6.0  # YouTube video transcript extraction
```

**Step 4: Install new dependencies**

Run: `cd backend && pip install -r requirements.txt`
Expected: Packages install successfully

**Step 5: Verify readers work**

Run: `cd backend && python3 -c "from agno.knowledge import Knowledge; k = Knowledge(); print('Readers ready:', k.website_reader is not None and k.youtube_reader is not None)"`
Expected: `Readers ready: True`

**Step 6: Commit**

```bash
git add backend/requirements.txt
git commit -m "deps: add website and YouTube reader dependencies

- beautifulsoup4 for HTML parsing
- requests for HTTP fetching
- youtube-transcript-api for video transcripts
- Required for Agno website_reader and youtube_reader"
```

---

## Task 5: Backend - Integration Test for YouTube Source

**Files:**
- Modify: `backend/tests/test_kb_multi_source.py:30-34`

**Step 1: Update the YouTube test**

Remove `@pytest.mark.skip` and implement the test in `backend/tests/test_kb_multi_source.py`:

```python
@pytest.mark.skipif(
    not os.environ.get("DATABASE_URL") or not os.environ.get("OPENAI_API_KEY"),
    reason="Requires DATABASE_URL and OPENAI_API_KEY for integration testing"
)
@pytest.mark.asyncio
async def test_knowledge_base_loads_youtube_source():
    """Test that Knowledge Base can load YouTube URLs."""
    from agno.knowledge import Knowledge
    from agno.vectordb.pgvector import PgVector

    # Create Knowledge instance
    vector_db = PgVector(
        table_name="kb_test_yt",
        db_url=os.environ["DATABASE_URL"]
    )
    knowledge = Knowledge(name="kb_test_yt", vector_db=vector_db)

    # Use a short, well-known YouTube video with captions
    # "Never Gonna Give You Up" by Rick Astley (has verified captions)
    youtube_url = "https://www.youtube.com/watch?v=dQw4w9WgXcQ"

    # Load YouTube video transcript
    await knowledge.add_content_async(url=youtube_url)

    # Verify no exception was raised
    assert True
```

**Step 2: Run test to verify it passes**

Run: `cd backend && OPENAI_API_KEY=$(grep OPENAI_API_KEY .env | cut -d= -f2) DATABASE_URL=$(grep DATABASE_URL .env | cut -d= -f2) pytest tests/test_kb_multi_source.py::test_knowledge_base_loads_youtube_source -v -s`
Expected: PASS (may take 10-20 seconds to fetch transcript and create embeddings)

**Step 3: Commit**

```bash
git add backend/tests/test_kb_multi_source.py
git commit -m "test: add YouTube source integration test

- Tests loading YouTube video transcripts
- Uses Rick Astley video (known to have captions)
- Verifies youtube-transcript-api integration"
```

---

## Task 6: Backend - Integration Test for Website Source

**Files:**
- Modify: `backend/tests/test_kb_multi_source.py:38-42`

**Step 1: Update the website test**

Remove `@pytest.mark.skip` and implement the test:

```python
@pytest.mark.skipif(
    not os.environ.get("DATABASE_URL") or not os.environ.get("OPENAI_API_KEY"),
    reason="Requires DATABASE_URL and OPENAI_API_KEY for integration testing"
)
@pytest.mark.asyncio
async def test_knowledge_base_loads_website_source():
    """Test that Knowledge Base can load website URLs."""
    from agno.knowledge import Knowledge
    from agno.vectordb.pgvector import PgVector

    # Create Knowledge instance
    vector_db = PgVector(
        table_name="kb_test_web",
        db_url=os.environ["DATABASE_URL"]
    )
    knowledge = Knowledge(name="kb_test_web", vector_db=vector_db)

    # Use a stable, simple website (Python.org about page)
    website_url = "https://www.python.org/about/"

    # Load website content
    await knowledge.add_content_async(url=website_url)

    # Verify no exception was raised
    assert True
```

**Step 2: Run test to verify it passes**

Run: `cd backend && OPENAI_API_KEY=$(grep OPENAI_API_KEY .env | cut -d= -f2) DATABASE_URL=$(grep DATABASE_URL .env | cut -d= -f2) pytest tests/test_kb_multi_source.py::test_knowledge_base_loads_website_source -v -s`
Expected: PASS (may take 5-10 seconds to fetch and process)

**Step 3: Commit**

```bash
git add backend/tests/test_kb_multi_source.py
git commit -m "test: add website source integration test

- Tests loading website content via URL
- Uses Python.org as stable test source
- Verifies beautifulsoup4 + requests integration"
```

---

## Task 7: Backend - E2E Test with Mixed Sources

**Files:**
- Create: `backend/tests/test_kb_mixed_sources_e2e.py`

**Step 1: Write the E2E test**

Create `backend/tests/test_kb_mixed_sources_e2e.py`:

```python
"""E2E test for Knowledge Base with mixed source types."""
import os
import pytest
from app.compiler.flow_compiler import FlowCompiler
from app.services.execution_engine import ExecutionEngine
from app.models.flow import FlowDefinition


@pytest.mark.skipif(
    not os.environ.get("DATABASE_URL") or not os.environ.get("OPENAI_API_KEY"),
    reason="Requires DATABASE_URL and OPENAI_API_KEY for E2E testing"
)
@pytest.mark.asyncio
async def test_kb_mixed_sources_e2e():
    """E2E test: Mixed sources (file + website + YouTube) ‚Üí RAG query."""

    # Create a test text file
    test_file = "/tmp/test_kb_mixed.txt"
    with open(test_file, "w") as f:
        f.write("CoconutFlow is a visual workflow builder for AI agents.")

    # Build flow: Input ‚Üí Knowledge Base (mixed sources) ‚Üí Output
    flow_definition = {
        "id": "test-kb-mixed",
        "name": "KB Mixed Sources E2E",
        "nodes": [
            {
                "id": "input-1",
                "type": "input",
                "position": {"x": 0, "y": 0},
                "config": {
                    "input_output": {
                        "label": "What is Python used for?",
                        "data_type": "text"
                    }
                }
            },
            {
                "id": "kb-1",
                "type": "knowledge_base",
                "position": {"x": 200, "y": 0},
                "config": {
                    "knowledge_base": {
                        "kb_type": "document",
                        "vector_db": "pgvector",
                        "sources": [
                            test_file,  # Local file
                            "https://www.python.org/about/",  # Website
                            # Skip YouTube for faster test (transcript fetching is slow)
                        ],
                        "chunk_size": 500,
                        "chunk_overlap": 50
                    }
                }
            },
            {
                "id": "output-1",
                "type": "output",
                "position": {"x": 400, "y": 0},
                "config": {"output": {}}
            }
        ],
        "edges": [
            {
                "id": "e1",
                "source": "input-1",
                "target": "kb-1",
                "sourceHandle": None,
                "targetHandle": None
            },
            {
                "id": "e2",
                "source": "kb-1",
                "target": "output-1",
                "sourceHandle": None,
                "targetHandle": None
            }
        ]
    }

    # Parse and compile the flow
    flow = FlowDefinition.model_validate(flow_definition)
    compiler = FlowCompiler()
    execution_graph = compiler.compile(flow)

    # Verify KB node was compiled with knowledge object
    kb_compiled = execution_graph["compiled_nodes"]["kb-1"]
    assert "knowledge" in kb_compiled
    assert kb_compiled["knowledge"] is not None

    # Execute the flow
    engine = ExecutionEngine()
    events = []

    async for event in engine.execute(execution_graph, user_input=""):
        events.append(event.to_dict())
        print(f"[{event.type}] {event.node_id}: {event.message or event.data or ''}")

    # Verify execution completed
    flow_complete = next(
        (e for e in events if e["type"] == "flow_complete"),
        None
    )
    assert flow_complete is not None

    # Verify KB node executed
    kb_output = next(
        (e for e in events if e["type"] == "node_output" and e["node_id"] == "kb-1"),
        None
    )
    assert kb_output is not None

    # Verify response mentions Python (from website content)
    response = kb_output["data"].lower()
    assert "python" in response

    # Cleanup
    os.remove(test_file)

    print("\n‚úÖ Mixed sources E2E test PASSED!")
    print(f"‚úÖ Loaded: 1 file + 1 website")
    print(f"‚úÖ RAG Response: {kb_output['data'][:200]}...")
```

**Step 2: Run test to verify it passes**

Run: `cd backend && OPENAI_API_KEY=$(grep OPENAI_API_KEY .env | cut -d= -f2) DATABASE_URL=$(grep DATABASE_URL .env | cut -d= -f2) pytest tests/test_kb_mixed_sources_e2e.py -v -s`
Expected: PASS (may take 15-30 seconds due to website fetching + embeddings)

**Step 3: Commit**

```bash
git add backend/tests/test_kb_mixed_sources_e2e.py
git commit -m "test: add E2E test for mixed KB sources

- Tests file + website in single Knowledge Base
- Verifies RAG query works across multiple source types
- Full pipeline: compile ‚Üí load ‚Üí embed ‚Üí query ‚Üí respond"
```

---

## Task 8: Update Documentation

**Files:**
- Modify: `docs/SUPABASE_SETUP.md:105-150`
- Modify: `CLAUDE.md:122-136`

**Step 1: Add multi-source section to SUPABASE_SETUP.md**

In `docs/SUPABASE_SETUP.md`, after line 105, add:

```markdown
## Multi-Source Support

CoconutFlow Knowledge Base supports mixing different source types in a single node:

### Supported Source Types

**Files (upload to backend):**
- `.pdf` - Research papers, documents
- `.txt` - Plain text files
- `.md` - Markdown documentation
- `.csv` - Data tables
- `.json` - Structured data
- `.docx` - Microsoft Word documents
- `.pptx` - PowerPoint presentations

**URLs (direct loading):**
- **Websites** - Any HTTP/HTTPS URL (e.g., `https://python.org/about/`)
  - Uses BeautifulSoup for HTML parsing
  - Extracts main content, ignores navigation/ads

- **YouTube Videos** - Video URLs (e.g., `https://youtube.com/watch?v=abc123`)
  - Extracts video transcripts/captions
  - Requires video to have available captions
  - Supports youtube.com and youtu.be URLs

### How It Works

The backend automatically detects source type:
- **File path** (local or uploaded) ‚Üí Uses file reader (PDF, TXT, PPTX, etc.)
- **HTTP/HTTPS URL** ‚Üí Uses website reader (BeautifulSoup)
- **YouTube URL** ‚Üí Uses YouTube transcript reader

All sources are:
1. Converted to text chunks (configurable size)
2. Embedded using OpenAI
3. Stored in Supabase pgvector
4. Searchable via RAG queries

### Example: Mixed Sources

```json
{
  "sources": [
    "/uploads/company-docs.pdf",
    "https://docs.python.org/3/tutorial/",
    "https://youtube.com/watch?v=tutorial-video-id"
  ]
}
```

All three sources become searchable in a single Knowledge Base!
```

**Step 2: Update CLAUDE.md tested patterns**

In `CLAUDE.md`, update the Knowledge Base line (around line 129):

```markdown
- [x] Knowledge Base RAG pipeline - COMPLETE
  - [x] Backend infrastructure + Supabase setup (Session Pooler for IPv4 compatibility)
  - [x] File upload with validation (binary rejection, size warnings)
  - [x] Document processing (generic Knowledge class, auto-detects TXT/PDF/MD/DOCX/PPTX)
  - [x] Multi-source support (files + websites + YouTube URLs)
  - [x] Async document loading in execution engine (avoids event loop conflicts)
  - [x] E2E testing with real documents, websites, and YouTube videos
```

**Step 3: Commit**

```bash
git add docs/SUPABASE_SETUP.md CLAUDE.md
git commit -m "docs: document multi-source Knowledge Base support

- Added supported file types (including .pptx)
- Documented website and YouTube URL support
- Explained auto-detection and mixed source usage
- Updated CLAUDE.md tested patterns"
```

---

## Task 9: Update Memory

**Files:**
- Modify: `/Users/affinitylabs/.claude/projects/-Users-affinitylabs-Downloads-coconut-coconutflow-main/memory/MEMORY.md:27-31`

**Step 1: Add multi-source lessons to memory**

Update the "Knowledge Base Lessons" section:

```markdown
## Knowledge Base Lessons
- **Supabase connection:** Use Session Pooler URL for IPv4 (`aws-1-eu-west-1.pooler.supabase.com`), not direct connection (`db.` subdomain requires IPv6)
- **Agno Knowledge class:** Create empty with `Knowledge(name=..., vector_db=...)`, then load docs async via `add_content_async(path=...)` in execution engine
- **Async/await:** Compiler is sync, execution engine is async ‚Äî defer slow ops (document embedding) to execution time
- **Input node config:** Use `config.input_output.label` not `config.input.value` (matches Pydantic model structure)
- **Multi-source support:** Knowledge.add_content_async() accepts both `path=...` (files) and `url=...` (websites/YouTube)
- **Source detection:** Pattern match on string: YouTube domains ‚Üí YOUTUBE, http(s):// ‚Üí WEBSITE, else ‚Üí FILE
- **Dependencies:** beautifulsoup4 + requests (website), youtube-transcript-api (YouTube)
```

**Step 2: Commit**

```bash
git add /Users/affinitylabs/.claude/projects/-Users-affinitylabs-Downloads-coconut-coconutflow-main/memory/MEMORY.md
git commit -m "docs: update memory with multi-source KB lessons

- Document source type detection pattern
- Note add_content_async dual usage (path vs url)
- List required dependencies for each source type"
```

---

## Verification Steps

After completing all tasks, run full test suite:

**1. Unit tests (fast, no API key needed):**
```bash
cd backend
pytest tests/ -v -k "not integration and not e2e"
```
Expected: All unit tests PASS (~20 tests)

**2. Integration tests (require DATABASE_URL + OPENAI_API_KEY):**
```bash
cd backend
OPENAI_API_KEY=$(grep OPENAI_API_KEY .env | cut -d= -f2) \
DATABASE_URL=$(grep DATABASE_URL .env | cut -d= -f2) \
pytest tests/test_kb_multi_source.py tests/test_kb_mixed_sources_e2e.py -v -s
```
Expected: All integration tests PASS (may take 30-60 seconds)

**3. Existing KB tests still pass:**
```bash
cd backend
OPENAI_API_KEY=$(grep OPENAI_API_KEY .env | cut -d= -f2) \
DATABASE_URL=$(grep DATABASE_URL .env | cut -d= -f2) \
pytest tests/test_kb_e2e.py -v
```
Expected: PASS

**4. Manual verification - Upload PPTX:**
```bash
# Create test PPTX
python3 -c "
import zipfile
with zipfile.ZipFile('test.pptx', 'w') as zf:
    zf.writestr('[Content_Types].xml', '<?xml version=\"1.0\"?><Types/>')
"

# Upload via API
curl -X POST http://localhost:8000/api/upload \
  -F "file=@test.pptx"

# Should return 200 with file_id
```

**5. Manual verification - YouTube source:**
```bash
# Test YouTube transcript extraction
cd backend
python3 -c "
from agno.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
import asyncio
import os

async def test():
    vector_db = PgVector(
        table_name='kb_manual_test',
        db_url=os.environ['DATABASE_URL']
    )
    k = Knowledge(name='test', vector_db=vector_db)
    await k.add_content_async(url='https://youtube.com/watch?v=dQw4w9WgXcQ')
    print('‚úÖ YouTube loading successful')

asyncio.run(test())
"
```

Expected: "‚úÖ YouTube loading successful"

---

## Summary

This plan implements **multi-source support** for Knowledge Base nodes:

**‚úÖ Completed Features:**
1. **PowerPoint (.pptx)** file uploads
2. **Website URL** loading with BeautifulSoup
3. **YouTube URL** loading with transcript API
4. **Unified interface** - mix files + URLs in single KB
5. **Auto-detection** - FILE vs WEBSITE vs YOUTUBE by pattern
6. **Full E2E testing** - all source types + mixed sources

**üéØ Architecture Highlights:**
- Backend-only changes (frontend URL input deferred per YAGNI)
- Source detection happens in execution engine (async context)
- Agno's built-in readers do the heavy lifting
- Tests cover all three source types + mixed usage

**üì¶ New Dependencies:**
- `beautifulsoup4` - HTML parsing
- `requests` - HTTP fetching
- `youtube-transcript-api` - Video transcripts

**‚è±Ô∏è Estimated Time:** 60-90 minutes for full implementation

**üöÄ Next Steps (Future):**
- Frontend URL input UI (currently file-upload only)
- Firecrawl reader for advanced web scraping
- PDF OCR for scanned documents
